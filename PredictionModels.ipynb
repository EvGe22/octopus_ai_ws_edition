{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "import json\n",
    "import pickle\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "from scipy import stats\n",
    "from pylab import rcParams\n",
    "from scipy.stats import randint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "rcParams['figure.figsize'] = (16, 9)\n",
    "matplotlib.rcParams['figure.figsize'] = (16, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/household_power_consumption.txt', sep=';', \n",
    "                   parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, \n",
    "                   low_memory=False, na_values=['nan','?'], index_col='dt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in data.columns:\n",
    "    plt.title(f'{column} (2 week duration)')\n",
    "    data[column][:60*24*14].plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для простоты будем работать с **дневным** потреблением электричества (*Global active power*), посмотрим внимательнее на данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[['Global_active_power']].resample('D').apply(sum).plot(title='Total GAP per day', figsize=(12, 8)) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание: Что не так с этим графиком?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данных пропусков примерно на 18 дней, интерполируем пропущенные значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.interpolate(method='time', inplace=True)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Global_active_power'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Global_active_power']].resample('D').apply(sum).plot(title='Total GAP per day', figsize=(12, 8)) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Простые бейзлайн решения для прогнозирования рядов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Посчитаем ошибки прогноза:\n",
    "MAPE (mean absolute percentage error) — это средняя абсолютная ошибка нашего прогноза. MAPE часто используется для оценки качества, поскольку эта величина относительная и по ней можно сравнивать качество даже на различных наборах данных.\n",
    "\n",
    "$y_i$ - значение ряда в момент времени $i$.  \n",
    "$\\hat{y_i}$ - прогноз нашей модели в то же время.  \n",
    "\n",
    "$e_i = y_i - \\hat{y_i}$ - ошибка прогноза.  \n",
    "$p_i = \\frac{e_i}{y_i}$ - относительная ошибка прогноза.\n",
    "\n",
    "$MAE = mean\\space (\\mid{e_i}\\mid)$  \n",
    "$MAPE = mean\\space (\\mid{p_i}\\mid)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание: реализовать функции для подсчёта MAE, MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_error(y_true, y_pred):\n",
    "    return <your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs_percentage_error(y_true, y_pred):\n",
    "     return <your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Прогнозирование вчерашним днём"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(gt, pred, last_n=60, title='Graph'):\n",
    "    plt.plot(pred[-last_n:], label='Prediction')\n",
    "    plt.plot(gt[-last_n:], label='Ground Truth')\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data['Global_active_power'].resample('D').apply(sum)\n",
    "prediction = df.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(df, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Naive MAE = {mean_abs_error(df[1:], prediction.dropna())}')\n",
    "print(f'Naive MAPE = {mean_abs_percentage_error(df[1:], prediction.dropna())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скользящее среднее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = df.rolling(7).apply(np.mean).shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(df, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Moving average MAE = {mean_abs_error(df[7:], prediction.dropna())}')\n",
    "print(f'Moving average MAPE = {mean_abs_percentage_error(df[7:], prediction.dropna())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экспоненциальные сглаживания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(series, alpha):\n",
    "    result = [series[0]] # first value is same as series\n",
    "    for n in range(1, len(series)):\n",
    "        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n",
    "    return pd.Series(result, index=series.index)\n",
    "\n",
    "prediction = exponential_smoothing(df, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(df, prediction.shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Exponential smoothing MAE = {mean_abs_error(df[1:], prediction.shift(1).dropna())}')\n",
    "print(f'Exponential smoothing MAPE = {mean_abs_percentage_error(df[1:], prediction.shift(1).dropna())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Взвешенное скользящее среднее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(series, weights):\n",
    "    if np.sum(weights) != 1:\n",
    "        raise ValueError('Weights must sum to 1')\n",
    "    weights.reverse()\n",
    "    window_size = len(weights)\n",
    "    w_fn = lambda x: np.sum(weights * x)\n",
    "    return series.rolling(window_size).apply(w_fn)\n",
    "\n",
    "prediction = weighted_average(df, [0.6, 0.2, 0.1, 0.07, 0.03])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(df, prediction.shift(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Weighted moving average MAE = {mean_abs_error(df[5:], prediction.shift(1).dropna())}')\n",
    "print(f'Weighted moving average MAPE = {mean_abs_percentage_error(df[5:], prediction.shift(1).dropna())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Краткая теория по временным рядам\n",
    "\n",
    "В классических задачах анализа данных предполагается что все наблюдение независимы, однако при прогнозировании временных рядов, мы наоборот, надеемся, что значения ряда в прошлом содержат информацию о его поведении в будущем.\n",
    "\n",
    "### Компоненты временных рядов:\n",
    "\n",
    "**Тренд** — плавное долгосрочное изменение уровня ряда.  \n",
    "**Сезонность** — циклические изменения уровня ряда с постоянным\n",
    "периодом.  \n",
    "**Цикл** — изменения уровня ряда с переменным периодом (экономические\n",
    "циклы, периоды солнечной активности).  \n",
    "**Ошибка** — непрогнозируемая случайная компонента ряда.\n",
    "\n",
    "\n",
    "### Стационарность:\n",
    "\n",
    "Под [**стационарностью**](https://ru.wikipedia.org/wiki/Стационарность) понимают свойство процесса не менять своих статистических характеристик с течением времени, а именно постоянство матожидания, постоянство дисперсии (она же [гомоскедастичность](https://ru.wikipedia.org/wiki/Гомоскедастичность)) и независимость ковариационной функции от времени (должна зависеть только от расстояния между наблюдениями).\n",
    "\n",
    "Для нас это означает, что временные ряды с трендом и сезонностью - нестационарны. Цикличность, тем не менее, оставляет ряд стационарным, потому что нельзя заранее предсказать, где будут находиться минимумы или максимумы ряда.  \n",
    "\n",
    "Почему стационарность так важна? По стационарному ряду просто строить прогноз, так как мы полагаем, что его будущие статистические характеристики не будут отличаться от наблюдаемых текущих. Большинство моделей временных рядов так или иначе моделируют и предсказывают эти характеристики (например, матожидание или дисперсию), поэтому в случае нестационарности исходного ряда предсказания окажутся неверными. К сожалению, большинство временных рядов, с которыми приходится сталкиваться за пределыми учебных материалов, стационарными не являются, но с этим можно (и нужно) бороться.\n",
    "\n",
    "Бороться с нестационарностью можно множеством способов - дифференцированием, выделением тренда и сезонности, сглаживаниями и различными преобразованиями (логарифмирование, Бокс-Кокс)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формально проверить гипотезу о стационарности ряда можно с помощью теста [Дики-Фуллера](https://ru.wikipedia.org/wiki/Тест_Дики_—_Фуллера). Однако всегда нужно смотреть на ряд глазами, потому что тесты в ряде случаях могут ошибаться. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание: Какие из представленных ниже временных рядов стационарны?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/download.png\"/>\n",
    "\n",
    "Знаменитая картинка из [лекции](https://www.youtube.com/watch?v=u433nrxdf5k) Евгения Рябенко о временных рядах. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Модель\n",
    "\n",
    "Немного о модели: [ARIMA](https://ru.wikipedia.org/wiki/ARIMA) (autoregressive integrated moving average). Существует [теорема Вольда](https://ru.wikipedia.org/wiki/%D0%A2%D0%B5%D0%BE%D1%80%D0%B5%D0%BC%D0%B0_%D0%92%D0%BE%D0%BB%D0%B4%D0%B0), которая говорит о том, что любой стационарный ряд может быть описан моделью ARIMA, а это значит, что наша задача в первую очередь привести ряд к стационарному виду, после чего производить моделирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['Global_active_power']].resample('D').apply(np.mean)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = sm.tsa.seasonal_decompose(df['Global_active_power'])\n",
    "fig = decomposition.plot()\n",
    "plt.show()\n",
    "\n",
    "print(\"Критерий Дики-Фуллера: p=%f\" % sm.tsa.stattools.adfuller(df['Global_active_power'])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобразование Бокса-Кокса\n",
    "\n",
    "Это монотонное преобразование для данных, которое обычно используется для стабилизации дисперсии. Подробнее [здесь.](https://en.wikipedia.org/wiki/Power_transform) \n",
    "\n",
    "Для исходной последовательности $y = \\{ y_1, \\ldots, y_n \\}, \\quad y_i > 0, \\quad i = 1,\\ldots,n$ однопараметрическое преобразование Бокса-Кокса с параметром $\\lambda$ определяется следующим образом:\n",
    "\n",
    "$y_i^{\\lambda} = \\begin{cases}\\frac{y_i^\\lambda-1}{\\lambda},&\\text{if } \\lambda \\neq 0,\\\\ \\log{(y_i)},& \\text{if } \\lambda = 0.\\end{cases}$\n",
    "\n",
    "Параметр $\\lambda$ можно выбирать, максимизируя логарифм правдоподобия. Еще один способ поиска оптимального значения параметра основан на поиске максимальной величины коэффициента корреляции между квантилями функции нормального распределения и отсортированной преобразованной последовательностью."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Global_active_power_log'] = np.log(df['Global_active_power'])\n",
    "df['Global_active_power_log'].plot();\n",
    "plt.ylabel(u'Logarithmic global active power')\n",
    "\n",
    "print(\"Критерий Дики-Фуллера: p=%f\" % sm.tsa.stattools.adfuller(df['Global_active_power_log'])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборочная автокорреляция - обычная корреляция между исходным рядом и его версией, сдвинутой на несколько отсчётов. Колличество отсчётов, на которое мы сдвигаем ряд называется **лагом автокорреляции**.  \n",
    "\n",
    "$$\\hat{\\rho}_k = \\frac{\\sum_{t=k+1}^{T}(y_t - \\bar{y})(y_{t-k}-\\bar{y})}{\\sum_{t=1}^{T}(y_t - \\bar{y})^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### В каком диапазоне лежат значения автокорреляции? :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборочная частичная автокорреляция:  \n",
    "\n",
    "$$\\hat{y}_t = * + * \\cdot y_{t-1} + * \\cdot y_{t_2} + \\cdots + * \\cdot y_{t-k+1} + \\phi_{k} \\cdot y_{t-k} + u_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(2, 1, 1)\n",
    "sm.graphics.tsa.plot_acf(df['Global_active_power_log'].dropna().values.squeeze(), lags=48, ax=ax)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "sm.graphics.tsa.plot_pacf(df['Global_active_power_log'].dropna().values.squeeze(), lags=48, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "print(\"Критерий Дики-Фуллера: p=%f\" % sm.tsa.stattools.adfuller(df['Global_active_power_log'].dropna())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Global_active_power_log_week'] = df['Global_active_power_log'] - df['Global_active_power_log'].shift(7)\n",
    "df['Global_active_power_log_diff_week'] = df['Global_active_power_log_week'] - df['Global_active_power_log_week'].shift(1)\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "sm.graphics.tsa.plot_acf(df['Global_active_power_log_week'].dropna().values.squeeze(), lags=96, ax=ax)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "sm.graphics.tsa.plot_pacf(df['Global_active_power_log_week'].dropna().values.squeeze(), lags=96, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "print(\"Критерий Дики-Фуллера: p=%f\" % sm.tsa.stattools.adfuller(df['Global_active_power_log_week'].dropna())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot(2, 1, 1)\n",
    "sm.graphics.tsa.plot_acf(df['Global_active_power_log_diff_week'].dropna().values.squeeze(), lags=96, ax=ax)\n",
    "\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "sm.graphics.tsa.plot_pacf(df['Global_active_power_log_diff_week'].dropna().values.squeeze(), lags=96, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "print(\"Критерий Дики-Фуллера: p=%f\" % sm.tsa.stattools.adfuller(df['Global_active_power_log_diff_week'].dropna())[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим в итоге на то, что у нас получилось.\n",
    "df['Global_active_power_log_diff_week'].dropna().plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps, Ps = range(0, ), range(0, )\n",
    "qs, Qs = range(0, ), range(0, )\n",
    "d, D = \n",
    "lag = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_ns = df['Global_active_power_log'].index.dayofyear\n",
    "day_ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_season = patsy.dmatrix('bs(day_ns, df=8)')\n",
    "print(exog_season.shape)\n",
    "print(exog_season[:5])\n",
    "\n",
    "exog_season2 = exog_season[:, 1:]  # bug/requirement: no constant in exog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portion = 31\n",
    "train_df, test_df = df[:-train_portion], df[-train_portion:]\n",
    "train_exog, test_exog = exog_season2[:-train_portion], exog_season2[-train_portion:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import warnings\n",
    "\n",
    "parameters = itertools.product(ps, qs, Ps, Qs)\n",
    "results = []\n",
    "best_aic = float(\"inf\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "for parameter in list(parameters):\n",
    "    # Модель обучается не на всех наборах параметров\n",
    "    try:\n",
    "        model=sm.tsa.statespace.SARIMAX(train_df['Global_active_power_log'], \n",
    "                                        order=(parameter[0], d, parameter[1]), \n",
    "                                        seasonal_order=(parameter[2], D, parameter[3], lag),\n",
    "                                        exog=train_exog).fit(disp=-1)\n",
    "    # Выводим параметры, на которых модель не обучается и переходим к следующему набору\n",
    "    except ValueError as e:\n",
    "        print('wrong parameters:', parameter)\n",
    "        continue\n",
    "    aic = model.aic\n",
    "    # Сохраняем лучшую модель, aic, параметры\n",
    "    if aic < best_aic:\n",
    "        best_model = model\n",
    "        best_aic = aic\n",
    "        best_parameter = parameter\n",
    "    results.append([parameter, model.aic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_table = pd.DataFrame(results)\n",
    "result_table.columns = ['parameters', 'aic']\n",
    "result_table.sort_values(by='aic', ascending=True).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_df['model'] = np.exp(best_model.fittedvalues)\n",
    "plot_prediction(train_df['Global_active_power'], train_df['model'], last_n=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_values = np.exp(best_model.forecast(31, exog=test_exog))\n",
    "plt.plot(np.concatenate([train_df['Global_active_power'].values[-200:], pred_values]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(test_df['Global_active_power'], pred_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SARIMAX MAE = {mean_abs_error(pred_values, test_df[\"Global_active_power\"].values)}')\n",
    "print(f'SARIMAX MAPE = {mean_abs_percentage_error(pred_values, test_df[\"Global_active_power\"].values)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarima_folder = os.path.join('models', 'sarima')\n",
    "!mkdir -p {sarima_folder}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.save(os.path.join(sarima_folder, 'model.sa'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(sarima_folder, 'exog'), test_exog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fbprophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet был разработан для прогнозирования большого числа различных бизнес-показателей и строит достаточно хорошие default'ные прогнозы. Кроме того, он дает возможность, изменяя человеко-понятные параметры, улучшать прогноз и не требует от аналитиков глубоких знаний устройства предсказательных моделей.\n",
    "\n",
    "Из минусов можно отметить достаточно небольшую точность предсказания \"из коробки\", для повышения качества придется повозиться с настройкой параметров.\n",
    "\n",
    "[Статья](https://research.fb.com/prophet-forecasting-at-scale/) от Facebook про Prophet.  \n",
    "[Ссылка](https://facebook.github.io/prophet/docs/quick_start.html) на официальную документацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека **Prophet** имеет интерфейс похожий на *sklearn*, сначала мы создаем модель, затем вызываем у нее метод *fit* для тренировки и *predict* для предсказания. На вход методу *fit* библиотека принимает *dataframe* с двумя колонками:\n",
    "\n",
    "**ds** — время, поле должно быть типа date или datetime,  \n",
    "**y** — числовой показатель, который мы хотим предсказывать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data[['Global_active_power']].copy()\n",
    "df = df.resample('D').apply(sum)\n",
    "df.reset_index(inplace=True)\n",
    "df.columns = ['ds', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_size = 31\n",
    "\n",
    "df = data[['Global_active_power']].copy()\n",
    "df = df.resample('D').apply(sum)\n",
    "df.reset_index(inplace=True)\n",
    "df.columns = ['ds', 'y']\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[:-prediction_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Prophet(daily_seasonality=True)\n",
    "model.fit(train_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того что бы построить предсказания методу *predict* нужно передать *dataframe* с количеством записей, равным периоду, на который нужно предсказать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dataframe = model.make_future_dataframe(periods=prediction_size, freq='D')\n",
    "future_dataframe.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model.predict(future_dataframe)\n",
    "forecast.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_components(forecast);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_comparison_dataframe(historical, forecast):\n",
    "    \"\"\"Join the history with the forecast.\n",
    "    \n",
    "       The resulting dataset will contain columns 'yhat', 'yhat_lower', 'yhat_upper' and 'y'.\n",
    "    \"\"\"\n",
    "    return forecast.set_index('ds')[['yhat', 'yhat_lower', 'yhat_upper']].join(historical.set_index('ds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_df = make_comparison_dataframe(df, forecast)\n",
    "\n",
    "mape = mean_abs_percentage_error(cmp_df['y'][-prediction_size:], cmp_df['yhat'][-prediction_size:])\n",
    "mae = mean_abs_error(cmp_df['y'][-prediction_size:], cmp_df['yhat'][-prediction_size:])\n",
    "\n",
    "print(f'Prophet MAPE = {mape},\\nProphet MAE = {mae}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(cmp_df['y'], cmp_df['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_boxcox(y, lambda_):\n",
    "    return np.exp(y) if lambda_ == 0 else np.exp(np.log(lambda_ * y + 1) / lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_boxcox = df.copy().set_index('ds')\n",
    "df_with_boxcox['y'], lambda_ = np.log(df_with_boxcox['y']), 0\n",
    "df_with_boxcox.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([df_with_boxcox, df.y], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_wb = df_with_boxcox[:-prediction_size]\n",
    "\n",
    "model_wb = Prophet(daily_seasonality=True)\n",
    "model_wb.fit(train_df_wb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_df_wb = model_wb.make_future_dataframe(periods=prediction_size, freq='D')\n",
    "future_df_wb.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_wb = model_wb.predict(future_df_wb)\n",
    "forecast_wb.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wb.plot(forecast_wb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wb.plot_components(forecast_wb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['yhat', 'yhat_lower', 'yhat_upper']:\n",
    "    forecast_wb[column] = inverse_boxcox(forecast_wb[column], lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_df_wb = make_comparison_dataframe(df, forecast_wb)\n",
    "mape_wb = mean_abs_percentage_error(cmp_df_wb['y'][-prediction_size:], cmp_df_wb['yhat'][-prediction_size:])\n",
    "mae_wb = mean_abs_error(cmp_df_wb['y'][-prediction_size:], cmp_df_wb['yhat'][-prediction_size:])\n",
    "\n",
    "print(f'MAPE = {mape:.2f}, MAPE with Box-Cox = {mape_wb:.2f}')\n",
    "print(f'MAE = {mae:.2f}, MAE with Box-Cox = {mae_wb:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_prediction(cmp_df_wb['y'], cmp_df_wb['yhat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./models/fbprophet/\n",
    "import pickle\n",
    "with open('models/fbprophet/model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNNS**\n",
    "<img src='./images/RNN_types.jpeg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/RNN.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./images/RNNnotation.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM - Long-Short-term Memory**\n",
    "\n",
    "Данный вид рекуррентных нейронных сетей позволяет сохранять как длительные, так и коротковременные зависимости в последовательностях данных. Это происходит за счёт особой архитектуры сети, которую вы можете увидеть ниже  \n",
    "\n",
    "<img src='./images/LSTM.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каждой ячейке имеется вход новых данных и вход предыдущего состояния. А также ячейка имеет свое \"состояние\", благодаря которому сеть и \"запоминает\" данные.  \n",
    "\n",
    "$$f = \\sigma(W_f [h_{t-1}; x_t] + b_f)$$  \n",
    "$$i = \\sigma(W_i [h_{t-1}; x_t] + b_i)$$  \n",
    "$$o = \\sigma(W_o [h_{t-1}; x_t] + b_o)$$  \n",
    "\n",
    "$$\\tilde c_{t} = tanh(W_h [h_{t-1}; x_t] + b_h)$$  \n",
    "$$c_t = f \\odot c_{t-1} + i \\odot \\tilde c_t$$  \n",
    "\n",
    "$$h_t = o \\odot tanh(c_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    dff = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(dff.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(dff.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_resample = data.resample('h').mean() \n",
    "df_resample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df_resample.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed = series_to_supervised(scaled, 7, 1)\n",
    "\n",
    "reframed.drop(reframed.columns[50:], axis=1, inplace=True)\n",
    "print(reframed.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = reframed.values\n",
    "\n",
    "n_train_time = 365*24*3\n",
    "train = values[:n_train_time, :]\n",
    "test = values[n_train_time:, :]\n",
    "\n",
    "# split into input and outputs\n",
    "train_X, train_y = train[:, :-1], train[:, -1]\n",
    "test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_X = train_X.reshape(<your code here>)\n",
    "test_X = test_X.reshape(<your code here>)\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(40, input_shape=(train_X.shape[1], train_X.shape[2]), return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(20, return_sequences=True))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(LSTM(10))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_X, train_y, epochs=20, batch_size=70,\n",
    "                    validation_data=(test_X, test_y), verbose=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "# summarize history for loss\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_predictions(predictions, test_X, test_y, scaler):\n",
    "    test_X_ = test_X[:, 0, :]\n",
    "    # invert scaling for forecast\n",
    "    inv_yhat = np.concatenate((predictions, test_X_[:, -6:]), axis=1)\n",
    "    inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "    inv_yhat = inv_yhat[:,0]\n",
    "    # invert scaling for actual\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    inv_y = np.concatenate((test_y, test_X_[:, -6:]), axis=1)\n",
    "    inv_y = scaler.inverse_transform(inv_y)\n",
    "    inv_y = inv_y[:,0]\n",
    "    return inv_y, inv_yhat\n",
    "    \n",
    "# make a prediction\n",
    "yhat = model.predict(test_X)\n",
    "inv_y, inv_yhat = convert_predictions(yhat, test_X, test_y, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(inv_y, inv_yhat, last_n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'LSTM MAE = {mean_abs_error(inv_y, inv_yhat)}')\n",
    "print(f'LSTM MAPE = {mean_abs_percentage_error(inv_y, inv_yhat)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, scaler, folder_name):\n",
    "    model_folder = os.path.join('models', folder_name)\n",
    "    model_json_path = os.path.join(model_folder, 'model.json')\n",
    "    weights_path = os.path.join(model_folder, 'weights.hd5')\n",
    "    scaler_path = os.path.join(model_folder, 'scaler.pkl')\n",
    "\n",
    "    !mkdir -p {model_folder}\n",
    "\n",
    "    with open(model_json_path, 'w') as f:\n",
    "        json.dump(model.to_json(), f)\n",
    "    model.save_weights(weights_path)\n",
    "\n",
    "    with open(scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, scaler, 'lstm_keras_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(dataset, n_train_time):\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled = scaler.fit_transform(dataset)\n",
    "    \n",
    "    reframed = series_to_supervised(scaled, 7, 1)\n",
    "    reframed.drop(reframed.columns[50:], axis=1, inplace=True)\n",
    "    \n",
    "    values = reframed.values\n",
    "\n",
    "    train = values[:n_train_time, :]\n",
    "    test = values[n_train_time:, :]\n",
    "\n",
    "    # split into input and outputs\n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], 7, 7))\n",
    "    test_X = test_X.reshape((test_X.shape[0], 7, 7))\n",
    "\n",
    "    model = create_lstm_model()\n",
    "    history = model.fit(train_X, train_y, epochs=20, batch_size=70,\n",
    "                        validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "    # summarize history for loss\n",
    "    plot_history(history)\n",
    "    \n",
    "    yhat = model.predict(test_X)\n",
    "    inv_y, inv_yhat = convert_predictions(yhat, test_X, test_y, scaler)    \n",
    "    plot_prediction(inv_y, inv_yhat, last_n=200)\n",
    "    \n",
    "    print(f'LSTM MAE = {mean_abs_error(inv_y, inv_yhat)}')\n",
    "    print(f'LSTM MAPE = {mean_abs_percentage_error(inv_y, inv_yhat)}\\n')\n",
    "    \n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, scaler = train_lstm(data.resample('d').mean(), 365*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, scaler, 'lstm_keras_d_mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, scaler = train_lstm(data.resample('d').sum(), 365*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, scaler, 'lstm_keras_d_sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, scaler = train_lstm(data.resample('h').mean(), 365*24*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGBM** это одна из самых популярных библиотек для градиентного бустинга. Основные её преимущества в том, что она эффективна по памяти и действительно быстрая (бонусом идёт возможность работать с большими массивами данных и sparse матрицами).\n",
    "\n",
    "[Ссылка](https://lightgbm.readthedocs.io/en/latest/) на официальную документацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание: придумайте признаки для модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_encoding(data, cat_feature, real_feature):\n",
    "    \"\"\"\n",
    "    Возвращает словарь, где ключами являются уникальные категории признака cat_feature, \n",
    "    а значениями - средние по real_feature\n",
    "    \"\"\"\n",
    "    return dict(data.groupby(cat_feature)[real_feature].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(data, target_column, lag_start=1, lag_end=2, test_size=0.15):\n",
    "    \n",
    "    data = pd.DataFrame(data.copy())\n",
    "    \n",
    "    # считаем индекс в датафрейме, после которого начинается тестовый отрезок\n",
    "    test_index = int(len(data) * (1 - test_size))\n",
    "    \n",
    "    # добавляем лаги исходного ряда в качестве признаков\n",
    "    for column in data.columns:\n",
    "        for i in range(lag_start, lag_end):\n",
    "            data[f\"{column}_t-{i}\"] = data[column].shift(i)\n",
    "        if column != target_column:\n",
    "            data.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "    data[\"hour\"] = data.index.hour\n",
    "    data[\"weekday\"] = data.index.weekday\n",
    "    data['is_weekend'] = data.weekday.isin([5, 6]) * 1\n",
    "    \n",
    "    # считаем средние только по тренировочной части, чтобы избежать лика\n",
    "    data[f'{target_column}_weekday_average'] = list(map(mean_encoding(data[:test_index], 'weekday',\n",
    "                                                                      target_column).get, data.weekday))\n",
    "    data[f\"{target_column}_hour_average\"] = list(map(mean_encoding(data[:test_index], 'hour',\n",
    "                                                                   target_column).get, data.hour))\n",
    "\n",
    "    group = list(data1.groupby(['hour', f\"{target_column}_hour_average\"]).indices)\n",
    "    h_mean = [m for h, m in group]\n",
    "    group = list(data1.groupby(['weekday', f\"{target_column}_weekday_average\"]).indices)\n",
    "    w_mean = [m for w, m in group]\n",
    "    \n",
    "    # выкидываем закодированные средними признаки\n",
    "    data.drop([\"hour\", \"weekday\"], axis=1, inplace=True)\n",
    "\n",
    "    \n",
    "    data = data.dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    \n",
    "    # разбиваем весь датасет на тренировочную и тестовую выборку\n",
    "    X_train = data.loc[:test_index].drop([target_column], axis=1)\n",
    "    y_train = data.loc[:test_index][target_column]\n",
    "    X_test = data.loc[test_index:].drop([target_column], axis=1)\n",
    "    y_test = data.loc[test_index:][target_column]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, w_mean, h_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, w_mean, h_mean = prepare_df(data.resample('h').mean(),\n",
    "                                                              \"Global_active_power\",\n",
    "                                                              lag_end=7,\n",
    "                                                              test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Логарифмируем таргет (частный случай преобразования Бокса-Кокса)\n",
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train_log)\n",
    "lgb_test = lgb.Dataset(X_test, y_test_log, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'learning_rate': 0.005,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                verbose_eval=False,\n",
    "                valid_sets=lgb_test,\n",
    "                num_boost_round=10000,\n",
    "                early_stopping_rounds=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Не забываем про обратное преобразование\n",
    "y_pred = np.exp(gbm.predict(X_test))\n",
    "\n",
    "print(f'GBM MAE = {mean_abs_error(y_test, y_pred)}')\n",
    "print(f'GBM MAPE = {mean_abs_percentage_error(y_test, y_pred)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(y_test.values, y_pred, last_n=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./models/light_gbm_h_mean/\n",
    "gbm.save_model('models/light_gbm_h_mean/weights.gbm');\n",
    "with open('models/light_gbm_h_mean/data.pkl', 'wb') as f:\n",
    "    pickle.dump((w_mean, h_mean), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дублируем то же самое на дневных суммах и дневных средних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgb(dataset):\n",
    "    X_train, X_test, y_train, y_test, w_mean, h_mean = prepare_df(dataset,\n",
    "                                                                  \"Global_active_power\",\n",
    "                                                                  lag_end=7,\n",
    "                                                                  test_size=0.15)\n",
    "    y_train_log = np.log(y_train)\n",
    "    y_test_log = np.log(y_test)\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train_log)\n",
    "    lgb_test = lgb.Dataset(X_test, y_test_log, reference=lgb_train)\n",
    "\n",
    "    params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'learning_rate': 0.005,\n",
    "        'verbose': 0\n",
    "    }\n",
    "\n",
    "    gbm = lgb.train(params,\n",
    "                    lgb_train,\n",
    "                    verbose_eval=False,\n",
    "                    valid_sets=lgb_test,\n",
    "                    num_boost_round=10000,\n",
    "                    early_stopping_rounds=50)\n",
    "    # Не забываем про обратное преобразование\n",
    "    y_pred = np.exp(gbm.predict(X_test))\n",
    "\n",
    "    print(f'GBM MAE = {mean_abs_error(y_test, y_pred)}')\n",
    "    print(f'GBM MAPE = {mean_abs_percentage_error(y_test, y_pred)}\\n')\n",
    "   \n",
    "    \n",
    "    return gbm, w_mean, h_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm, w_mean, h_mean = train_lgb(data.resample('D').mean())\n",
    "!mkdir -p ./models/light_gbm_d_mean/\n",
    "gbm.save_model('models/light_gbm_d_mean/weights.gbm');\n",
    "with open('models/light_gbm_d_mean/data.pkl', 'wb') as f:\n",
    "    pickle.dump((w_mean, h_mean), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm, w_mean, h_mean = train_lgb(data.resample('D').sum())\n",
    "!mkdir -p ./models/light_gbm_d_sum/\n",
    "gbm.save_model('models/light_gbm_d_sum/weights.gbm');\n",
    "with open('models/light_gbm_d_sum/data.pkl', 'wb') as f:\n",
    "    pickle.dump((w_mean, h_mean), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
